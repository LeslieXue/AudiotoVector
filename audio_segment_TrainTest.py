import sys
sys.path.append('util')
import os, glob, json, random
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from Transformer import TransformerEncoderModel
from util import point_to_index, index_to_point


# Dataset ç±»
class ArrayDataset(Dataset):
    def __init__(self, samples):
        self.samples = samples
    def __len__(self):
        return len(self.samples)
    def __getitem__(self, idx):
        x, y = self.samples[idx]
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)

# è§„èŒƒåˆ—åï¼šå»é™¤ BOM/é›¶å®½ç©ºæ ¼/ä¸é—´æ–­ç©ºæ ¼ï¼Œå¹¶ strip
def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    cleaned = []
    for c in df.columns:
        s = str(c)
        s = s.replace("\ufeff", "")   # BOM
        s = s.replace("\u200b", "")   # zero-width space
        s = s.replace("\xa0", " ")    # non-breaking space
        s = s.strip()
        cleaned.append(s)
    df.columns = cleaned
    return df

# å·¥å…·å‡½æ•°ï¼šè§£æ JSON å‘é‡åˆ—
def parse_vector_column(series, expected_len=11):
    def parse_string(s):
        try:
            val = json.loads(s)
            arr = np.array(val)
            if arr.shape == (expected_len,):
                return arr
        except Exception:
            pass
        try:
            s_clean = s.strip().strip('"').strip("'")
            if not s_clean.startswith("["):
                s_clean = "[" + s_clean
            if not s_clean.endswith("]"):
                s_clean = s_clean + "]"
            arr = np.array(json.loads(s_clean))
            if arr.shape == (expected_len,):
                return arr
            else:
                print(f"âš ï¸ å°ºå¯¸ä¸ç¬¦: æœŸæœ› {expected_len}, å®é™… {arr.shape} â† å†…å®¹: {s}")
        except Exception as e:
            print(f"âŒ æ— æ³•è§£æ: {s}ï¼Œé”™è¯¯: {e}")
        return np.array([])
    return series.apply(parse_string)

def load_samples(data_folder, allowed_labels=None, point=61):
    all_csv_paths = glob.glob(os.path.join(data_folder, "*.csv"))
    if not all_csv_paths:
        from audio_segment_DataGen import get_segment_vectors
        print(f"âš ï¸ No CSV files found in {data_folder}, generating with get_segment_vectors...")
        get_segment_vectors(input_folder=data_folder)
        all_csv_paths = glob.glob(os.path.join(data_folder, "*.csv"))
    all_records = []
    for path in all_csv_paths:
        df = pd.read_csv(path, encoding="utf-8-sig", engine="python", on_bad_lines="skip")
        df = _normalize_columns(df)
        records = df.to_dict(orient="records")
        all_records.extend(records)

    if allowed_labels is not None:
        all_records = [r for r in all_records if r["shift_fre_point"] in allowed_labels]

    df = pd.DataFrame(all_records)
    for col in ["period_wave_abs_diff", 
                "normalized_energy_vector_sum",
                "normalized_energy_vector_squared",
                "cosine_similarity_vector"]:
        if col in df.columns:
            df[col] = parse_vector_column(df[col].astype(str), expected_len=point)

    samples = []
    for _, row in df.iterrows():
        vectors = [row["period_wave_abs_diff"],
                   row["normalized_energy_vector_sum"],
                   row["normalized_energy_vector_squared"],
                   row["cosine_similarity_vector"]]
        if all(v.shape == (point,) for v in vectors):
            features = np.stack(vectors, axis=1)
            label = int(row["shift_fre_point"])
            samples.append((features, label))
    return samples

#def load_samples_from_paths(file_paths, allowed_labels=None, point=61):
    all_records = []
    for path in file_paths:
        df = pd.read_csv(path, encoding="utf-8-sig", engine="python", on_bad_lines="skip")
        df = _normalize_columns(df)
        records = df.to_dict(orient="records")
        all_records.extend(records)

    if allowed_labels is not None:
        all_records = [r for r in all_records if r["shift_fre_point"] in allowed_labels]

    df = pd.DataFrame(all_records)
    for col in ["period_wave_abs_diff", 
                "normalized_energy_vector_sum",
                "normalized_energy_vector_squared",
                "cosine_similarity_vector"]:
        if col in df.columns:
            df[col] = parse_vector_column(df[col].astype(str), expected_len=point)

    samples = []
    for _, row in df.iterrows():
        vectors = [row["period_wave_abs_diff"],
                   row["normalized_energy_vector_sum"],
                   row["normalized_energy_vector_squared"],
                   row["cosine_similarity_vector"]]
        if all(v.shape == (point,) for v in vectors):
            features = np.stack(vectors, axis=1)
            label = int(row["shift_fre_point"])
            samples.append((features, label))
    return samples

def load_samples_from_paths(file_paths, allowed_labels=None, point=61):
    all_records = []
    for path in file_paths:
        df = pd.read_csv(path)
        df = _normalize_columns(df)
        print(f"ğŸ“„ æ­£åœ¨å¤„ç†æ–‡ä»¶: {path}ï¼Œæ€»è¡Œæ•°: {len(df)}")
        # æ£€æŸ¥å‘é‡åˆ—æ˜¯å¦ä¸ºåˆæ³• JSON æ ¼å¼
        for col in ["period_wave_abs_diff", 
                    "normalized_energy_vector_sum",
                    "normalized_energy_vector_squared",
                    "cosine_similarity_vector"]:
            invalid_json_count = 0
            for i, val in enumerate(df[col].astype(str)):
                try:
                    arr = json.loads(val)
                    if not isinstance(arr, list):
                        raise ValueError("ä¸æ˜¯åˆ—è¡¨")
                except Exception as e:
                    print(f"âŒ éæ³• JSON: {col} ç¬¬ {i} è¡Œ å†…å®¹: {val}ï¼Œé”™è¯¯: {e}")
                    invalid_json_count += 1
            print(f"ğŸ” {col} åˆ—éæ³• JSON æ•°é‡: {invalid_json_count}")
        records = df.to_dict(orient="records")
        all_records.extend(records)

    if allowed_labels is not None:
        before = len(all_records)
        all_records = [r for r in all_records if r.get("shift_fre_point") in allowed_labels]
        after = len(all_records)
        print(f"âœ… ç­›é€‰ shift_fre_point æˆåŠŸ: {after}/{before} æ¡è®°å½•ä¿ç•™")

    df = pd.DataFrame(all_records)

    for col in ["period_wave_abs_diff", 
                "normalized_energy_vector_sum",
                "normalized_energy_vector_squared",
                "cosine_similarity_vector"]:
        if col not in df.columns:
            print(f"âŒ ç¼ºå°‘åˆ—: {col}")
            continue
        df[col] = parse_vector_column(df[col], expected_len=point)

    samples = []
    for _, row in df.iterrows():
        vectors = [row["period_wave_abs_diff"],
                   row["normalized_energy_vector_sum"],
                   row["normalized_energy_vector_squared"],
                   row["cosine_similarity_vector"]]
        print("vectors = ", [v.shape for v in vectors], "|", [v.tolist() for v in vectors])
        if all(v.shape == (point,) for v in vectors):
            label = int(row["shift_fre_point"])
            features = np.stack(vectors, axis=1)
            samples.append((features, label))
        else:
            print(f"âš ï¸ å‘é‡ç»´åº¦ä¸åŒ¹é…: {[v.shape for v in vectors]}")

    print(f"âœ… æœ‰æ•ˆæ ·æœ¬æ•°: {len(samples)}")
    return samples

def split_data(samples, train_ratio=0.7, val_ratio=0.1):
    random.shuffle(samples)
    n = len(samples)
    train_end = int(train_ratio * n)
    val_end = train_end + int(val_ratio * n)
    train_samples = samples[:train_end]
    val_samples = samples[train_end:val_end]
    test_samples = samples[val_end:]
    return train_samples, val_samples, test_samples

# -------------------- è®­ç»ƒå‡½æ•° --------------------
def train(train_samples, val_samples, model_path="f1_model.pt", max_samples=None, nc = 61):
    if max_samples is not None and max_samples < len(train_samples):
        train_samples = random.sample(train_samples, max_samples)

    train_loader = DataLoader(ArrayDataset(train_samples), batch_size=32, shuffle=True)
    val_loader = DataLoader(ArrayDataset(val_samples), batch_size=32)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = TransformerEncoderModel(input_dim=4, embed_dim=32, num_heads=4, num_layers=2, num_classes= nc, dropout=0.1)
    model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.0001)

    for epoch in range(10):
        model.train()
        total_loss = 0
        correct_train, total_train = 0, 0
        for x, y in train_loader:
            if torch.isnan(x).any() or torch.isinf(x).any():
                print("âŒ è¾“å…¥ x å­˜åœ¨ NaN æˆ– Inf")
            if torch.isnan(y).any() or torch.isinf(y).any():
                print("âŒ æ ‡ç­¾ y å­˜åœ¨ NaN æˆ– Inf")
            x, y = x.to(device), y.to(device)
            optimizer.zero_grad()
            out = model(x)
            if torch.isnan(out).any() or torch.isinf(out).any():
                print("âŒ æ¨¡å‹è¾“å‡ºå­˜åœ¨ NaN æˆ– Inf")
                print(f"â†’ è¾“å…¥ x = {x[0]}")
                continue

            loss = criterion(out, y)
            if torch.isnan(loss) or torch.isinf(loss):
                print("âŒ æŸå¤±å‡½æ•°ä¸º NaN æˆ– Inf")
                print(f"â†’ æ ‡ç­¾ y = {y}")
                print(f"â†’ æ¨¡å‹è¾“å‡º out = {out}")
                continue

            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            pred = torch.argmax(out, dim=1)
            correct_train += (pred == y).sum().item()
            total_train += y.size(0)

        train_acc = correct_train / total_train if total_train > 0 else 0

        # éªŒè¯
        model.eval()
        correct_val, total_val = 0, 0
        with torch.no_grad():
            for x_val, y_val in val_loader:
                x_val, y_val = x_val.to(device), y_val.to(device)
                out_val = model(x_val)
                pred_val = torch.argmax(out_val, dim=1)
                correct_val += (pred_val == y_val).sum().item()
                total_val += y_val.size(0)
        val_acc = correct_val / total_val if total_val > 0 else 0

        print(f"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")

    torch.save(model.state_dict(), model_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {model_path}")

# -------------------- æµ‹è¯•å‡½æ•° --------------------
def test(test_samples, model_path="f1_model.pt", nc = 61):
    test_loader = DataLoader(ArrayDataset(test_samples), batch_size=32)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = TransformerEncoderModel(input_dim=4, embed_dim=32, num_heads=4, num_layers=2, num_classes= nc, dropout=0.1)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()

    correct, total = 0, 0
    all_true, all_pred = [], []

    with torch.no_grad():
        for x, y in test_loader:
            x, y = x.to(device), y.to(device)
            out = model(x)
            pred = torch.argmax(out, dim=1)
            correct += (pred == y).sum().item()
            total += y.size(0)
            all_true.extend(y.cpu().numpy())
            all_pred.extend(pred.cpu().numpy())

    acc = correct / total if total > 0 else 0
    print(f"\nâœ… Test Accuracy: {acc:.4f}")

    # æ··æ·†çŸ©é˜µ
    all_true = np.array(all_true)
    all_pred = np.array(all_pred)
    unique_labels = sorted(set(all_true) | set(all_pred))
    label_names = [str(l) for l in unique_labels]
    label2idx = {l: i for i, l in enumerate(unique_labels)}

    conf_mat = np.zeros((len(unique_labels), len(unique_labels)), dtype=int)
    for t, p in zip(all_true, all_pred):
        conf_mat[label2idx[p], label2idx[t]] += 1

    df_count = pd.DataFrame(conf_mat, 
                            index=[f'Pred_{l}' for l in label_names], 
                            columns=[f'True_{l}' for l in label_names])
    print("\nğŸ“Š æ•°é‡ç»Ÿè®¡è¡¨ï¼š")
    print(df_count)

    col_sums = conf_mat.sum(axis=0, keepdims=True)
    df_ratio = df_count / col_sums
    print("\nğŸ“ˆ æ¯”ä¾‹ç»Ÿè®¡è¡¨ï¼š")
    print(df_ratio.round(3))

    print(f"\nğŸ“¦ æµ‹è¯•æ ·æœ¬æ€»æ•°: {len(all_true)}")


'''
if __name__ == "__main__":
    
    data_folder = "features_normal_key_3shift_11point"
    allowed_labels = [5, 12, 24]
    allowed_label_indices = [point_to_index(p) for p in allowed_labels]
    point = 11
    model_path = "f3_model.pt"
    all_csv_paths = glob.glob(os.path.join(data_folder, "*.csv"))
    random.seed(42)
    random.shuffle(all_csv_paths)
    half = len(all_csv_paths) // 2
    selected_train_files = all_csv_paths[:half]
    selected_test_files = all_csv_paths[half:]
    
    print(f"ğŸ” é€‰ä¸­è®­ç»ƒæ–‡ä»¶æ•°: {len(selected_train_files)}")
    print(f"ğŸ” é€‰ä¸­æµ‹è¯•æ–‡ä»¶æ•°: {len(selected_test_files)}")

    print("ğŸš€ åŠ è½½è®­ç»ƒæ ·æœ¬...")
    train_samples = load_samples_from_paths(selected_train_files, allowed_labels=allowed_labels, point=point)
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_samples)}")

    # é™åˆ¶æ ·æœ¬æ•°é‡ä¸ºæœ€å¤š 50000 æ¡
    max_train_samples = 50000
    if len(train_samples) > max_train_samples:
        train_samples = random.sample(train_samples, max_train_samples)

    # æŒ‰æ¯”ä¾‹åˆ’åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
    train_samples, val_samples, _ = split_data(train_samples, train_ratio=0.7, val_ratio=0.1)

    print("ğŸš€ åŠ è½½æµ‹è¯•æ ·æœ¬...")
    test_samples = load_samples_from_paths(selected_test_files, allowed_labels=allowed_labels, point=point)
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_samples)}")
    
    train_labels = [y for _, y in train_samples]
    print(f"è®­ç»ƒé›†ä¸­å®é™…ä½¿ç”¨çš„æ ‡ç­¾ç§ç±»: {sorted(set(train_labels))}")

    train(train_samples=train_samples, val_samples=val_samples, model_path=model_path, nc=11)
    test(test_samples=test_samples, model_path=model_path, nc=11)
    '''